{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d051e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsahn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize MLflow\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"Job_Domain_Classification\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Classifier\"):\n",
    "    grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best Random Forest model\n",
    "    best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Log model and parameters\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"best_rf_model\")\n",
    "    mlflow.log_params(grid_search_rf.best_params_)\n",
    "    rf_predictions = best_rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, rf_predictions))\n",
    "    print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Gradient_Boosting_Classifier\"):\n",
    "    gbm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(gbm_classifier, \"gbm_model\")\n",
    "    gbm_predictions = gbm_classifier.predict(X_test)\n",
    "    gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", gbm_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, gbm_predictions))\n",
    "    print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Support_Vector_Machine_Classifier\"):\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(svm_classifier, \"svm_model\")\n",
    "    svm_predictions = svm_classifier.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", svm_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, svm_predictions))\n",
    "    print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Predict domains for the original dataset\n",
    "data[\"predicted_domain\"] = best_rf_classifier.predict(vectorizer.transform(data[\"cleaned_text\"]))\n",
    "\n",
    "# Save the dataset with predictions to a new CSV file\n",
    "data.to_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a684082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.13.2)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (5.3.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: entrypoints<1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.1.43)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.4.1)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.24.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.25.0)\n",
      "Requirement already satisfied: packaging<25 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (23.0)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.5.3)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (4.24.4)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (15.0.2)\n",
      "Requirement already satisfied: pytz<2025 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (2022.7.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (6.0)\n",
      "Requirement already satisfied: querystring-parser<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (2.28.2)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.2.2)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (1.10.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (2.0.30)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (0.5.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.1.2)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click<9,>=7.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (305)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (1.26.14)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Flask<4->mlflow) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from graphene<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from graphene<4->mlflow) (9.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.46b0)\n",
      "Requirement already satisfied: six in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.14.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64c4502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/14 12:46:32 INFO mlflow.tracking.fluent: Experiment with name 'Job_Domain_Classification' does not exist. Creating a new experiment.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024/06/14 12:46:37 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsahn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'mlflow' has no attribute 'log_confusion_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m     rf_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, rf_predictions)\n\u001b[0;32m    141\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_accuracy)\n\u001b[1;32m--> 142\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_confusion_matrix\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m, confusion_matrix(y_test, rf_predictions))\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_accuracy)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient_Boosting_Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mlflow' has no attribute 'log_confusion_matrix'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"Job_Domain_Classification\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Classifier\"):\n",
    "    grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best Random Forest model\n",
    "    best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Log model and parameters\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"best_rf_model\")\n",
    "    mlflow.log_params(grid_search_rf.best_params_)\n",
    "    rf_predictions = best_rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, rf_predictions))\n",
    "    print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Gradient_Boosting_Classifier\"):\n",
    "    gbm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(gbm_classifier, \"gbm_model\")\n",
    "    gbm_predictions = gbm_classifier.predict(X_test)\n",
    "    gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", gbm_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, gbm_predictions))\n",
    "    print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Support_Vector_Machine_Classifier\"):\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(svm_classifier, \"svm_model\")\n",
    "    svm_predictions = svm_classifier.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", svm_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, svm_predictions))\n",
    "    print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Predict domains for the original dataset\n",
    "data[\"predicted_domain\"] = best_rf_classifier.predict(vectorizer.transform(data[\"cleaned_text\"]))\n",
    "\n",
    "# Save the dataset with predictions to a new CSV file\n",
    "data.to_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d85c83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd76610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'mlflow' has no attribute 'log_confusion_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m     rf_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, rf_predictions)\n\u001b[0;32m    141\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_accuracy)\n\u001b[1;32m--> 142\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_confusion_matrix\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m, confusion_matrix(y_test, rf_predictions))\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_accuracy)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient_Boosting_Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mlflow' has no attribute 'log_confusion_matrix'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"Job_Domain_Classification\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Classifier\"):\n",
    "    grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best Random Forest model\n",
    "    best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Log model and parameters\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"best_rf_model\")\n",
    "    mlflow.log_params(grid_search_rf.best_params_)\n",
    "    rf_predictions = best_rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, rf_predictions))\n",
    "    print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Gradient_Boosting_Classifier\"):\n",
    "    gbm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(gbm_classifier, \"gbm_model\")\n",
    "    gbm_predictions = gbm_classifier.predict(X_test)\n",
    "    gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", gbm_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, gbm_predictions))\n",
    "    print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Support_Vector_Machine_Classifier\"):\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(svm_classifier, \"svm_model\")\n",
    "    svm_predictions = svm_classifier.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", svm_accuracy)\n",
    "    mlflow.log_confusion_matrix(\"confusion_matrix\", confusion_matrix(y_test, svm_predictions))\n",
    "    print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Predict domains for the original dataset\n",
    "data[\"predicted_domain\"] = best_rf_classifier.predict(vectorizer.transform(data[\"cleaned_text\"]))\n",
    "\n",
    "# Save the dataset with predictions to a new CSV file\n",
    "data.to_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa4c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsahn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024/06/16 13:41:45 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsahn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9945054945054945\n",
      "Gradient Boosting Machine Accuracy: 0.978021978021978\n",
      "Support Vector Machine Accuracy: 0.9945054945054945\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"Job_Domain_Classification\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, artifact_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_rf_classifier.classes_)\n",
    "    disp.plot()\n",
    "    plt.savefig(artifact_name)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(artifact_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Classifier\"):\n",
    "    grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best Random Forest model\n",
    "    best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Log model and parameters\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"best_rf_model\")\n",
    "    mlflow.log_params(grid_search_rf.best_params_)\n",
    "    rf_predictions = best_rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
    "    \n",
    "    # Log confusion matrix as an image artifact\n",
    "    log_confusion_matrix(y_test, rf_predictions, \"confusion_matrix_rf.png\")\n",
    "    \n",
    "    print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Gradient_Boosting_Classifier\"):\n",
    "    gbm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(gbm_classifier, \"gbm_model\")\n",
    "    gbm_predictions = gbm_classifier.predict(X_test)\n",
    "    gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", gbm_accuracy)\n",
    "    \n",
    "    # Log confusion matrix as an image artifact\n",
    "    log_confusion_matrix(y_test, gbm_predictions, \"confusion_matrix_gbm.png\")\n",
    "    \n",
    "    print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Support_Vector_Machine_Classifier\"):\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(svm_classifier, \"svm_model\")\n",
    "    svm_predictions = svm_classifier.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", svm_accuracy)\n",
    "    \n",
    "    # Log confusion matrix as an image artifact\n",
    "    log_confusion_matrix(y_test, svm_predictions, \"confusion_matrix_svm.png\")\n",
    "    \n",
    "    print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Predict domains for the original dataset\n",
    "data[\"predicted_domain\"] = best_rf_classifier.predict(vectorizer.transform(data[\"cleaned_text\"]))\n",
    "\n",
    "# Save the dataset with predictions to a new CSV file\n",
    "data.to_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7f4349",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2385425841.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    mlflow ui\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mlflow ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad67ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.13.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (5.3.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: entrypoints<1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.1.43)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.24.3)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.25.0)\n",
      "Requirement already satisfied: packaging<25 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (24.1)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (2.0.2)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (4.24.4)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (15.0.2)\n",
      "Requirement already satisfied: pytz<2025 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (2023.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (6.0.1)\n",
      "Requirement already satisfied: querystring-parser<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.5.0)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (2.0.30)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (0.5.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.1.4)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click<9,>=7.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (306)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.0.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Flask<4->mlflow) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphene<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from graphene<4->mlflow) (9.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.46b0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3->mlflow) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.17.3->mlflow) (2023.5.7)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hsahn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94da8926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b7f38",
   "metadata": {},
   "source": [
    "# Use of MLflow\n",
    "\n",
    "Use of MLflow:\n",
    "MLflow is used in this script to track and log experiments systematically:\n",
    "\n",
    "Experiment Setup:\n",
    "\n",
    "mlflow.set_experiment(\"Job_Domain_Classification\") ensures all runs are grouped under a specific experiment.\n",
    "Starting a Run:\n",
    "\n",
    "mlflow.start_run(run_name=\"Random_Forest_Classifier\"): Each model training is enclosed within an mlflow.start_run context. This starts a new run for logging.\n",
    "Logging Models and Parameters:\n",
    "\n",
    "mlflow.sklearn.log_model(best_rf_classifier, \"best_rf_model\"): Logs the trained model.\n",
    "mlflow.log_params(grid_search_rf.best_params_): Logs the best hyperparameters found during grid search.\n",
    "Logging Metrics:\n",
    "\n",
    "mlflow.log_metric(\"accuracy\", rf_accuracy): Logs the accuracy of the model on the test set.\n",
    "Logging Artifacts:\n",
    "\n",
    "mlflow.log_artifact(\"confusion_matrix_rf.png\"): Logs the confusion matrix as an image artifact.\n",
    "Benefits of Using MLflow:\n",
    "Experiment Tracking:\n",
    "\n",
    "MLflow provides a systematic way to track and organize experiments, including different runs, hyperparameters, metrics, and artifacts.\n",
    "Reproducibility:\n",
    "\n",
    "By logging all details of each run, it ensures that experiments are reproducible.\n",
    "Model Management:\n",
    "\n",
    "MLflow helps in managing different versions of models, making it easy to compare and select the best-performing models.\n",
    "Visualization:\n",
    "\n",
    "The MLflow UI provides a user-friendly interface to visualize and compare different runs, metrics, and artifacts.\n",
    "By integrating MLflow into the machine learning pipeline, the code not only performs model training and evaluation but also systematically logs all relevant details, making it easier to manage and track the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ec122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Random Forest Accuracy: 0.9945054945054945\n",
      "Gradient Boosting Machine Accuracy: 0.978021978021978\n",
      "Support Vector Machine Accuracy: 0.9945054945054945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"Job_Domain_Classification\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, artifact_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_rf_classifier.classes_)\n",
    "    disp.plot()\n",
    "    plt.savefig(artifact_name)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(artifact_name)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Classifier\"):\n",
    "    grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best Random Forest model\n",
    "    best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Log model and parameters\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"best_rf_model\")\n",
    "    mlflow.log_params(grid_search_rf.best_params_)\n",
    "    rf_predictions = best_rf_classifier.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
    "    \n",
    "    # Log confusion matrix as an image artifact\n",
    "    log_confusion_matrix(y_test, rf_predictions, \"confusion_matrix_rf.png\")\n",
    "    \n",
    "    print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Gradient_Boosting_Classifier\"):\n",
    "    gbm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(gbm_classifier, \"gbm_model\")\n",
    "    gbm_predictions = gbm_classifier.predict(X_test)\n",
    "    gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", gbm_accuracy)\n",
    "    \n",
    "    # Log confusion matrix as an image artifact\n",
    "    log_confusion_matrix(y_test, gbm_predictions, \"confusion_matrix_gbm.png\")\n",
    "    \n",
    "    print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Support_Vector_Machine_Classifier\"):\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    mlflow.sklearn.log_model(svm_classifier, \"svm_model\")\n",
    "    svm_predictions = svm_classifier.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "    mlflow.log_metric(\"accuracy\", svm_accuracy)\n",
    "    \n",
    "    # Log confusion matrix as an image artifact\n",
    "    log_confusion_matrix(y_test, svm_predictions, \"confusion_matrix_svm.png\")\n",
    "    \n",
    "    print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf935aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
