{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2118a7ad",
   "metadata": {},
   "source": [
    "# Function for Job_Candidate_RelevancyMatching_OpenAI \n",
    "\n",
    "1. **Function and Library Imports**: The code imports necessary libraries like pandas for data manipulation, json for handling JSON data, sklearn for calculating cosine similarity, transformers and torch for handling BERT embeddings, and openai for accessing OpenAI's API.\n",
    "\n",
    "\n",
    "2. **Function Definitions**:\n",
    "   - **`extract_job_details`**: This function takes a job description and an OpenAI API key as inputs. It uses OpenAI's API to extract detailed job requirements like eligibility criteria, skills, and experiences required for the job.\n",
    "   - **`parse_resume`**: This function takes a resume text and an OpenAI API key as inputs. It uses OpenAI's API to extract skills, experiences, projects, and certifications from the resume.\n",
    "   - **`get_bert_embedding`**: This function takes text, a BERT tokenizer, and a BERT model as inputs. It converts the text into a BERT embedding vector, which is used for calculating the similarity between job descriptions and candidate profiles.\n",
    "   - **`combine_skills`**: This function combines various skill-related fields from the candidate's data into a single string, making it easier to process and compare.\n",
    "   - **`extract_text_from_json`**: This helper function extracts specific text fields from JSON objects within the data.\n",
    "   \n",
    "   \n",
    "3. **Main Function - `find_relevant_candidates`**:\n",
    "   - **Loading Data**: The function reads the student database CSV and the job description CSV into pandas DataFrames.\n",
    "   - **Processing Candidate Data**: It processes each candidate's resume to extract and combine skills and experiences into a single text string. It then generates BERT embeddings for these combined texts.\n",
    "   - **Processing Job Descriptions**: It processes each job description to extract detailed job requirements using OpenAI. It then generates BERT embeddings for the combined job descriptions and requirements.\n",
    "   - **Calculating Relevancy Scores**: The function calculates cosine similarity scores between the BERT embeddings of the job descriptions and candidate profiles. These scores represent how relevant each candidate is to each job.\n",
    "   - **Selecting Top Candidates**: For each job, the function selects the top 15 candidates with the highest relevancy scores.\n",
    "   - **Saving Results**: It saves the job roles, along with the top 15 relevant candidates' names, emails, and relevancy scores, into a specified output CSV file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e13005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "def extract_job_details(job_description, openai_api_key):\n",
    "    openai.api_key = openai_api_key\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"Extract the job description, eligibility criteria including college, year of passing, branch, skills, and experiences required from the following job description: {job_description}\",\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def parse_resume(resume_text, openai_api_key):\n",
    "    openai.api_key = openai_api_key\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"Extract skills, experiences, projects, and certifications from the following resume: {resume_text}\",\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return json.loads(response.choices[0].text.strip())\n",
    "\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n",
    "\n",
    "def combine_skills(row, json_columns):\n",
    "    text = ''\n",
    "    for column in json_columns:\n",
    "        if column in ['HardSkills', 'SoftSkills']:\n",
    "            if isinstance(row[column], list):\n",
    "                text += ' '.join([skill['skill'] for skill in row[column] if 'skill' in skill]) + ' '\n",
    "        else:\n",
    "            text += extract_text_from_json(row[column], 'description') + ' '\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_json(json_field, key):\n",
    "    if isinstance(json_field, list):\n",
    "        return ' '.join([str(item[key]) for item in json_field if key in item])\n",
    "    return ''\n",
    "\n",
    "def find_relevant_candidates(job_description_csv, student_database_csv, openai_api_key, output_csv_path):\n",
    "    candidates_data = pd.read_csv(student_database_csv)\n",
    "    job_details_data = pd.read_csv(job_description_csv)\n",
    "\n",
    "    json_columns = ['Experiences', 'Projects', 'Achievements', 'Certifications', 'HardSkills', 'SoftSkills']\n",
    "\n",
    "    for column in json_columns:\n",
    "        candidates_data[column] = candidates_data[column].apply(json.loads)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    candidates_data['combined_skills'] = candidates_data.apply(lambda row: combine_skills(row, json_columns), axis=1)\n",
    "    candidates_data['embedding'] = candidates_data['combined_skills'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "\n",
    "    job_details_data['combined_job_text'] = job_details_data.apply(\n",
    "        lambda row: f\"{row['job_description']} {extract_job_details(row['job_description'], openai_api_key)}\", axis=1)\n",
    "    job_details_data['embedding'] = job_details_data['combined_job_text'].apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "\n",
    "    candidate_embeddings = candidates_data['embedding'].tolist()\n",
    "    job_embeddings = job_details_data['embedding'].tolist()\n",
    "\n",
    "    relevancy_scores = pd.DataFrame(index=candidates_data.index, columns=job_details_data.index)\n",
    "    for job_index, job_vector in enumerate(job_embeddings):\n",
    "        similarity_scores = cosine_similarity(candidate_embeddings, [job_vector])\n",
    "        relevancy_scores[job_index] = similarity_scores.flatten()\n",
    "\n",
    "    job_candidates_df = pd.DataFrame(columns=['JobRole', 'RelevantCandidates'])\n",
    "    for job_index in relevancy_scores.columns:\n",
    "        top_candidates = relevancy_scores[job_index].nlargest(15)\n",
    "        relevant_candidates = []\n",
    "        for candidate_index, score in top_candidates.items():\n",
    "            candidate_details = {\n",
    "                'CandidateName': candidates_data.loc[candidate_index, 'Name'],\n",
    "                'CandidateEmail': candidates_data.loc[candidate_index, 'Email'],\n",
    "                'RelevancyScore': score\n",
    "            }\n",
    "            relevant_candidates.append(candidate_details)\n",
    "        job_candidates_df = job_candidates_df.append({\n",
    "            'JobRole': job_details_data.loc[job_index, 'job_name'],\n",
    "            'RelevantCandidates': json.dumps(relevant_candidates)\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    job_candidates_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Job relevancy scores with relevant candidates saved to '{output_csv_path}'\")\n",
    "\n",
    "# Usage example:\n",
    "job_description_csv = \"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\"\n",
    "student_database_csv = \"C:/Users/hsahn/OneDrive/Desktop/all_resumes_data.csv\"\n",
    "openai_api_key = \"your_openai_api_key\"\n",
    "output_csv_path = \"C:/Users/hsahn/Downloads/job_relevancy_scores_with_candidates.csv\"\n",
    "\n",
    "find_relevant_candidates(job_description_csv, student_database_csv, openai_api_key, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf50a65",
   "metadata": {},
   "source": [
    "## I will check the code once the Open AI API key is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720ee67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
