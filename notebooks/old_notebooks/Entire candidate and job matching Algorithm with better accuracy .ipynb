{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d87365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hsahn\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15d4ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Random Forest Accuracy: 0.9945054945054945\n",
      "Gradient Boosting Machine Accuracy: 0.978021978021978\n",
      "Support Vector Machine Accuracy: 0.9945054945054945\n",
      "Confusion Matrix for Random Forest:\n",
      "[[20  0  0  0  0  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Confusion Matrix for Gradient Boosting Machine:\n",
      "[[17  0  0  1  2  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Confusion Matrix for Support Vector Machine:\n",
      "[[20  0  0  0  0  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Data has been converted to CSV format and saved as 'C:/Users/hsahn/OneDrive/Desktop/all_resumes_data.csv'\n",
      "Unique predicted domains in jobs_df: ['software development' 'data science' 'sales' 'product management'\n",
      " 'other' 'marketing' 'human resources' 'research' 'engineering']\n",
      "Unique parsed domains in candidates_exploded_df: ['data science' 'engineering' 'product management' 'operations' nan]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "# Train other classifiers without hyperparameter tuning\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "# predictions (continued)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# model performance\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Predict domains for the original dataset\n",
    "data[\"predicted_domain\"] = best_rf_classifier.predict(vectorizer.transform(data[\"cleaned_text\"]))\n",
    "\n",
    "# Save the dataset with predictions to a new CSV file\n",
    "data.to_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\", index=False)\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to process each resume JSON and flatten the data\n",
    "# Function to process each resume JSON and flatten the data\n",
    "def process_resume(resume_data):\n",
    "    flattened_data = {\n",
    "        \"Name\": resume_data.get(\"Name\", \"\"),\n",
    "        \"Email\": resume_data.get(\"Contact\", {}).get(\"Email\", \"\").lower(),\n",
    "        \"Phone\": resume_data.get(\"Contact\", {}).get(\"Phone\", \"\"),\n",
    "        \"LinkedIn\": resume_data.get(\"Contact\", {}).get(\"LinkedIn\", \"\"),\n",
    "        \"Github\": resume_data.get(\"Contact\", {}).get(\"GitHub\", \"\"),\n",
    "        \"Degree\": resume_data[\"Education\"].get(\"Degree\", \"\"),\n",
    "        \"Major\": resume_data[\"Education\"].get(\"Major\", \"\"),\n",
    "        \"Year\": resume_data[\"Education\"].get(\"Graduation Year\", \"\"),\n",
    "        \"CGPA\": resume_data[\"Education\"].get(\"CGPA\", \"\"),\n",
    "    }\n",
    "\n",
    "    # Combine all experiences into one cell\n",
    "    experiences = [{\"role\": exp.get(\"Role\", \"\"), \"company\": exp.get(\"Company\", \"\"), \"duration\": exp.get(\"Duration\", \"\"), \"description\": exp.get(\"Description\", \"\")} for exp in resume_data.get(\"UserExperience\", [])]\n",
    "    flattened_data[\"Experiences\"] = json.dumps(experiences)\n",
    "\n",
    "    # Combine all projects into one cell\n",
    "    projects = [{\"title\": project.get(\"Title\", \"\"), \"duration\": project.get(\"Duration\", \"\"), \"description\": project.get(\"Description\", \"\")} for project in resume_data.get(\"Projects\", [])]\n",
    "    flattened_data[\"Projects\"] = json.dumps(projects)\n",
    "\n",
    "    # Combine all achievements into one cell\n",
    "    achievements = [{\"achievement\": achievement} for achievement in resume_data.get(\"Achievements\", [])]\n",
    "    flattened_data[\"Achievements\"] = json.dumps(achievements)\n",
    "\n",
    "    # Combine all certifications into one cell if it is a list\n",
    "    if isinstance(resume_data.get(\"Certifications\"), list):\n",
    "        certifications = [{\"certification\": cert} for cert in resume_data.get(\"Certifications\", [])]\n",
    "    else:\n",
    "        certifications = []\n",
    "    flattened_data[\"Certifications\"] = json.dumps(certifications)\n",
    "\n",
    "    # Combine all hard skills into one cell\n",
    "    hard_skills = [{\"skill\": skill, \"percentage\": percentage} for skill, percentage in resume_data.get(\"Skills\", {}).get(\"Hard Skills\", {}).items()]\n",
    "    flattened_data[\"HardSkills\"] = json.dumps(hard_skills)\n",
    "\n",
    "    # Combine all soft skills into one cell\n",
    "    soft_skills = [{\"skill\": skill, \"percentage\": percentage} for skill, percentage in resume_data.get(\"Skills\", {}).get(\"Soft Skills\", {}).items()]\n",
    "    flattened_data[\"SoftSkills\"] = json.dumps(soft_skills)\n",
    "\n",
    "    # Combine all recommended job domains into one cell\n",
    "    recommended_job_domains = [{\"job_domain\": job_domain} for job_domain in resume_data.get(\"Recommended_Job_Domains\", [])]\n",
    "    flattened_data[\"RecommendedJobDomains\"] = json.dumps(recommended_job_domains)\n",
    "\n",
    "    return flattened_data\n",
    "\n",
    "\n",
    "# Function to read multiple resume JSON files and convert them to a consolidated CSV\n",
    "def convert_resumes_to_csv(directory_path, output_csv):\n",
    "    all_resumes = []\n",
    "    txt_files = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
    "\n",
    "    for file_path in txt_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            try:\n",
    "                resume_data = json.load(f)\n",
    "                flattened_data = process_resume(resume_data)\n",
    "                all_resumes.append(flattened_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON from file: {file_path}\")\n",
    "\n",
    "    if all_resumes:\n",
    "        df = pd.DataFrame(all_resumes)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Data has been converted to CSV format and saved as '{output_csv}'\")\n",
    "    else:\n",
    "        print(\"No valid resume data found.\")\n",
    "\n",
    "# Directory containing the text files with resume JSON data\n",
    "# Update the path below to point to your specific folder in the Downloads directory\n",
    "directory_path = \"C:/Users/hsahn/OneDrive/Desktop/resume data (json)\"  # For Windows\n",
    "# directory_path = \"/Users/YourUsername/Downloads/YourFolderName\"  # For macOS\n",
    "# directory_path = \"/home/YourUsername/Downloads/YourFolderName\"  # For Linux\n",
    "\n",
    "# Output CSV file\n",
    "output_csv = \"C:/Users/hsahn/OneDrive/Desktop/all_resumes_data.csv\"\n",
    "\n",
    "# Convert all resumes to CSV\n",
    "convert_resumes_to_csv(directory_path, output_csv)\n",
    "\n",
    "\n",
    "# Load the pre-trained models and vectorizer\n",
    "best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the dataset with predictions\n",
    "jobs_df = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\")\n",
    "\n",
    "candidates_df = pd.read_csv(\"C:/Users/hsahn/OneDrive/Desktop/all_resumes_data.csv\")\n",
    "\n",
    "def parse_job_domains(json_str):\n",
    "    try:\n",
    "        json_data = json.loads(json_str)\n",
    "        if isinstance(json_data, list):\n",
    "            domains = [item.get('job_domain', 'Other') for item in json_data]\n",
    "            return domains\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return ['Other']\n",
    "\n",
    "candidates_df['parsed_domains'] = candidates_df['RecommendedJobDomains'].apply(parse_job_domains)\n",
    "\n",
    "candidates_exploded_df = candidates_df.explode('parsed_domains')\n",
    "\n",
    "jobs_df['predicted_domain'] = jobs_df['predicted_domain'].str.strip().str.lower()\n",
    "candidates_exploded_df['parsed_domains'] = candidates_exploded_df['parsed_domains'].str.strip().str.lower()\n",
    "\n",
    "print(\"Unique predicted domains in jobs_df:\", jobs_df['predicted_domain'].unique())\n",
    "print(\"Unique parsed domains in candidates_exploded_df:\", candidates_exploded_df['parsed_domains'].unique())\n",
    "\n",
    "# Matching candidates to jobs based on fuzzy matching of domains\n",
    "# Matching candidates to jobs based on fuzzy matching of domains\n",
    "matched_candidates = defaultdict(list)\n",
    "\n",
    "for job_domain in jobs_df['predicted_domain'].unique():\n",
    "    for candidate_domain in candidates_exploded_df['parsed_domains'].unique():\n",
    "        # Convert the domains to strings before calculating the similarity ratio\n",
    "        job_domain_str = str(job_domain)\n",
    "        candidate_domain_str = str(candidate_domain)\n",
    "        \n",
    "        if fuzz.ratio(job_domain_str, candidate_domain_str) >= 80:\n",
    "            matched_candidates[job_domain].append(candidate_domain)\n",
    "\n",
    "# Creating a DataFrame to store matched jobs and candidates\n",
    "matched_data = []\n",
    "\n",
    "for job_domain, candidate_domains in matched_candidates.items():\n",
    "    for candidate_domain in candidate_domains:\n",
    "        job_matches = jobs_df[jobs_df['predicted_domain'] == job_domain]\n",
    "        candidate_matches = candidates_exploded_df[candidates_exploded_df['parsed_domains'] == candidate_domain]\n",
    "\n",
    "        for _, job_row in job_matches.iterrows():\n",
    "            for _, candidate_row in candidate_matches.iterrows():\n",
    "                matched_data.append({\n",
    "                    'company_name': job_row['company_name'],\n",
    "                    'role_title': job_row['role_title'],\n",
    "                    'role_description': job_row['role_description'],\n",
    "                    'predicted_domain': job_row['predicted_domain'],\n",
    "                    'Name': candidate_row['Name'],\n",
    "                    'Email': candidate_row['Email'],\n",
    "                    'Experiences': candidate_row['Experiences'],\n",
    "                    'parsed_domains': candidate_row['parsed_domains']\n",
    "                })\n",
    "\n",
    "# Creating DataFrame from matched data\n",
    "matched_df = pd.DataFrame(matched_data)\n",
    "\n",
    "# Saving the matched jobs and candidates to a CSV file\n",
    "matched_df.to_csv(\"C:/Users/hsahn/OneDrive/Desktop/matched_jobs_candidates.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178379e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
