{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d371dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12e52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ca0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list = [\n",
    "    \"python\", \"java\", \"kotlin\", \"jetpack compose\", \"android sdk\", \"firebase\",\n",
    "    \"rest\", \"json\", \"proto\", \"sql\", \"javascript\", \"cloud computing\", \"aws\",\n",
    "    \"excel\", \"data visualization\", \"react\", \"node.js\", \"marketing\", \"social media\",\n",
    "    \"seo\", \"content creation\", \"product management\", \"sales\", \"business development\",\n",
    "    \"hr\", \"research\", \"operations\", \"analytical skills\", \"problem solving\",\n",
    "    \"communication\", \"collaboration\", \"organizational skills\", \"multitasking\",\n",
    "    \"microsoft office\", \"ai\", \"machine learning\", \"big data\", \"deep learning\",\n",
    "    \"neural networks\", \"statistical analysis\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "    \"tensorflow\", \"keras\", \"r\", \"sas\", \"sql\", \"tableau\", \"power bi\",\n",
    "    \"lead generation\", \"b2b\", \"b2c\", \"market research\", \"product marketing\",\n",
    "    \"email marketing\", \"content strategy\", \"creative writing\", \"employee engagement\",\n",
    "    \"talent management\", \"recruitment\", \"project management\", \"agile\", \"scrum\",\n",
    "    \"supply chain management\", \"logistics\", \"procurement\", \"inventory management\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e885dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_fields = [\"role_description\", \"requirement\", \"description\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b951eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(text_fields) > 1:\n",
    "    data[\"combined_text\"] = data[text_fields].fillna(\"\").apply(lambda x: \" \".join(x), axis=1)\n",
    "    text_field = \"combined_text\"  # Use the combined field for further processing\n",
    "else:\n",
    "    text_field = text_fields[0]  # Use the single chosen field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c83e88a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [word for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb33bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleaned_text\"] = data[text_field].apply(clean_text)  # Apply cleaning (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac2ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter keywords based on the skill list\n",
    "def filter_keywords(text):\n",
    "    tokens = text.split()\n",
    "    return [word.lower() for word in tokens if word.lower() in skills_list]\n",
    "\n",
    "# Extract keywords\n",
    "data[\"keywords\"] = data[\"cleaned_text\"].apply(filter_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b730616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create one-hot encoded feature vectors\n",
    "def create_one_hot_vector(keywords, vocabulary):\n",
    "    vector = [0] * len(vocabulary)\n",
    "    for keyword in keywords:\n",
    "        if keyword in vocabulary:\n",
    "            vector[vocabulary.index(keyword)] = 1\n",
    "    return vector\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = list(set([word for skill in skills_list for word in skill.split(\" \")]))\n",
    "\n",
    "# Create one-hot encoded feature vectors\n",
    "data[\"features\"] = data[\"keywords\"].apply(lambda x: create_one_hot_vector(x, vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63008554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_domain(job_title):\n",
    "    for domain, titles in job_domains.items():\n",
    "        if job_title in titles:\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# Assign domains to each job\n",
    "data[\"domain\"] = data[\"role_title\"].apply(assign_domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83136467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[[35]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "feature_df = pd.DataFrame(data[\"features\"].to_list())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, data[\"domain\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Classification (Naive Bayes example)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1b4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted domain for new job: Other\n"
     ]
    }
   ],
   "source": [
    "# Classify new job details (example)\n",
    "new_text = \"\"\"Human Resource Intern Currently pursuing a degree in Human Resources Business Administration or a related field Strong interpersonal and communication skills Ability to maintain confidentiality and handle sensitive information with discretion Detail oriented with excellent organizational and multitasking abilities Proficient in Microsoft Office Suite Word Excel PowerPoint Enthusiastic and eager to learn about various aspects of human resources Currently pursuing a degree in Human Resources Business Administration or a related field Strong interpersonal and communication skills Ability to maintain confidentiality and handle sensitive information with discretion Detail oriented with excellent organizational and multitasking abilities Proficient in Microsoft Office Suite Word Excel PowerPoint Enthusiastic and eager to learn about various aspects of human resources Internship Job Description Human Resource Intern Internship Job Description Human Resource Intern About the Role About the Role As a Human Resource Intern at Nexus Grove you will have the unique opportunity to gain hands on experience in the dynamic field of human resources You will work closely with our HR team to support various aspects of the recruitment and employee lifecycle processes This internship is designed to provide exposure to the diverse facets of HR allowing you to develop key skills and knowledge that will lay a solid foundation for a successful career in human resources Key Responsibilities Key Responsibilities Assist in the recruitment process by sourcing and screening candidates conducting initial interviews and coordinating interview schedules Support the onboarding process for new hires ensuring a smooth transition into the organization Maintain accurate and up to date employee records including documentation related to personnel changes promotions and terminations Collaborate with the HR team to create and update HR policies and procedures Handle administrative tasks such as preparing HR related documents managing calendars and responding to internal and external inquiries Assist in the recruitment process by sourcing and screening candidates conducting initial interviews and coordinating interview schedules Support the onboarding process for new hires ensuring a smooth transition into the organization Maintain accurate and up to date employee records including documentation related to personnel changes promotions and terminations Collaborate with the HR team to create and update HR policies and procedures Handle administrative tasks such as preparing HR related documents managing calendars and responding to internal and external inquiries Benefits Benefits Gain practical experience in a professional HR environment Mentorship opportunities with seasoned HR professionals Exposure to diverse HR functions providing a comprehensive understanding of the field Networking opportunities within the HR industry Gain practical experience in a professional HR environment Mentorship opportunities with seasoned HR professionals Exposure to diverse HR functions providing a comprehensive understanding of the field Networking opportunities within the HR industry.\n",
    "\"\"\"\n",
    "new_vector = create_one_hot_vector(filter_keywords(clean_text(new_text)), vocabulary)\n",
    "predicted_domain = classifier.predict([new_vector])[0]\n",
    "print(\"Predicted domain for new job:\", predicted_domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7ab6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m---> 22\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_field\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Define job domains\u001b[39;00m\n\u001b[0;32m     25\u001b[0m job_domains \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftware Development\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mandroid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull stack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode.js\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb developer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     ]\n\u001b[0;32m     66\u001b[0m }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[0;32m     16\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     18\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([char \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m char\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     19\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") # Replace \"your_dataset.csv\" with the path to your CSV file\n",
    "\n",
    "text_field = \"role_description\"  # Assuming \"role_description\" contains the job descriptions\n",
    "\n",
    "# Download NLTK stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [word for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(clean_text)\n",
    "\n",
    "# Define job domains\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Assign domains based on job descriptions and keywords\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, keywords in job_domains.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# Assign domains based on job descriptions\n",
    "data[\"domain\"] = data[\"cleaned_text\"].apply(assign_domain)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"cleaned_text\"], data[\"domain\"], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test_vec)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfaaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") # Replace \"your_dataset.csv\" with the path to your CSV file\n",
    "\n",
    "text_field = \"role_description\"  # Assuming \"role_description\" contains the job descriptions\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):  # Check if the text is NaN\n",
    "        return \"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [word for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(clean_text)\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, keywords in job_domains.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[\"cleaned_text\"].apply(assign_domain)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"cleaned_text\"], data[\"domain\"], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test_vec)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") # Replace \"your_dataset.csv\" with the path to your CSV file\n",
    "\n",
    "text_field = \"role_description\"  # Assuming \"role_description\" contains the job descriptions\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize PorterStemmer for stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# function for text cleaning with stemming and modified stopwords handling\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):  # Check if the text is NaN\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(clean_text)\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, keywords in job_domains.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[\"cleaned_text\"].apply(assign_domain)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"cleaned_text\"], data[\"domain\"], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))  # Using unigrams and bigrams\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test_vec)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acdba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") # Replace \"your_dataset.csv\" with the path to your CSV file\n",
    "\n",
    "text_field = \"role_description\"  # Assuming \"role_description\" contains the job descriptions\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# function to calculate word count and sentence length\n",
    "def extract_features(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_count = len(words)\n",
    "    sentence_length = len(nltk.sent_tokenize(text))\n",
    "    return word_count, sentence_length\n",
    "\n",
    "# feature extraction\n",
    "data[\"word_count\"], data[\"sentence_length\"] = zip(*data[text_field].apply(extract_features))\n",
    "\n",
    "# Concatenate Word2Vec embeddings for each word in the job description\n",
    "def get_word_embeddings(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            embeddings.append(word2vec_model[word])\n",
    "    return embeddings\n",
    "\n",
    "# Apply Word2Vec embeddings\n",
    "data[\"word_embeddings\"] = data[text_field].apply(get_word_embeddings)\n",
    "\n",
    "# Flatten the list of word embeddings into a single feature vector\n",
    "data[\"word_embeddings\"] = data[\"word_embeddings\"].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[[\"word_count\", \"sentence_length\", \"word_embeddings\"]]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[[\"word_count\", \"sentence_length\"]])\n",
    "X_test_scaled = scaler.transform(X_test[[\"word_count\", \"sentence_length\"]])\n",
    "\n",
    "# Combine scaled numerical features with Word2Vec embeddings\n",
    "X_train_final = [list(X_train_scaled[i]) + X_train[\"word_embeddings\"].iloc[i] for i in range(len(X_train_scaled))]\n",
    "X_test_final = [list(X_test_scaled[i]) + X_test[\"word_embeddings\"].iloc[i] for i in range(len(X_test_scaled))]\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_final, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test_final)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fcb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90511fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") \n",
    "\n",
    "text_field = \"role_description\" \n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def extract_features(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_count = len(words)\n",
    "    sentence_length = len(nltk.sent_tokenize(text))\n",
    "    return word_count, sentence_length\n",
    "\n",
    "data[\"word_count\"], data[\"sentence_length\"] = zip(*data[text_field].apply(extract_features))\n",
    "\n",
    "# Concatenate Word2Vec embeddings for each word in the job description\n",
    "def get_word_embeddings(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in word2vec_model:\n",
    "            embeddings.append(word2vec_model[word])\n",
    "    return embeddings\n",
    "\n",
    "# Apply Word2Vec embeddings\n",
    "data[\"word_embeddings\"] = data[text_field].apply(get_word_embeddings)\n",
    "\n",
    "# Flatten the list of word embeddings into a single feature vector\n",
    "data[\"word_embeddings\"] = data[\"word_embeddings\"].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[[\"word_count\", \"sentence_length\", \"word_embeddings\"]]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[[\"word_count\", \"sentence_length\"]])\n",
    "X_test_scaled = scaler.transform(X_test[[\"word_count\", \"sentence_length\"]])\n",
    "\n",
    "X_train_final = [list(X_train_scaled[i]) + X_train[\"word_embeddings\"].iloc[i] for i in range(len(X_train_scaled))]\n",
    "X_test_final = [list(X_test_scaled[i]) + X_test[\"word_embeddings\"].iloc[i] for i in range(len(X_test_scaled))]\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_final, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test_final)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12784293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") \n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [word for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(clean_text)\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "rf_classifier = RandomForestClassifier()\n",
    "gbm_classifier = GradientBoostingClassifier()\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accc070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") \n",
    "\n",
    "text_field = \"role_description\"  \n",
    "\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(clean_text)\n",
    "\n",
    "data = data.dropna(subset=['cleaned_text'])\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the model and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d39179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") \n",
    "text_field = \"role_description\"  \n",
    "\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e24502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")  # Replace with your dataset path\n",
    "\n",
    "text_field = \"role_description\"  # Assuming \"role_description\" contains the job descriptions\n",
    "\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handling class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd92ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")  \n",
    "\n",
    "text_field = \"role_description\"  \n",
    "\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")  \n",
    "\n",
    "text_field = \"role_description\"  \n",
    "\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55416d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") \n",
    "\n",
    "text_field = \"role_description\"  \n",
    "\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handling class imbalance using SMOTE\n",
    "# Adjust k_neighbors to be smaller or use SMOTE with k_neighbors=1 to avoid errors with very small classes\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222e6cd",
   "metadata": {},
   "source": [
    "# Model Details-\n",
    "\n",
    "Data Preparation\n",
    "Loading Data: The data is loaded from a CSV file containing job details.\n",
    "Dropping Missing Values: Rows with missing values in the 'role_description' column are removed to ensure data quality.\n",
    "NLTK Resources: NLTK resources like stopwords, punkt, and wordnet are downloaded for text processing tasks.\n",
    "Lemmatizer Initialization: A WordNetLemmatizer is initialized for reducing words to their base form.\n",
    "\n",
    "Domain Assignment\n",
    "Job Domains Dictionary: A dictionary maps job domains to associated keywords.\n",
    "Assign Domain Function: This function assigns a domain to each job description by checking if any of the keywords in the job domains dictionary appear in the text.\n",
    "\n",
    "Text Cleaning\n",
    "Clean Text Function: This function cleans the text by:\n",
    "Converting it to lowercase.\n",
    "Removing non-alphanumeric characters.\n",
    "Removing stopwords.\n",
    "Lemmatizing the remaining words.\n",
    "Applying Text Cleaning: The cleaning function is applied to the 'role_description' column to create a new 'cleaned_text' column.\n",
    "\n",
    "Feature Extraction and Class Balancing\n",
    "Vectorization: The cleaned text data is vectorized using TF-IDF with bi-grams and a maximum of 1000 features.\n",
    "Handling Class Imbalance: RandomOverSampler is used to oversample the minority classes to balance the dataset.\n",
    "\n",
    "Train-Test Split\n",
    "The resampled data is split into training and testing sets with an 80-20 ratio.\n",
    "\n",
    "Model Training and Hyperparameter Tuning\n",
    "Random Forest Classifier:\n",
    "Hyperparameter Tuning: GridSearchCV is used to find the best hyperparameters for the RandomForestClassifier.\n",
    "Parameters Tuned: Number of estimators, maximum depth, and minimum samples split.\n",
    "Gradient Boosting Classifier: Trained with default parameters.\n",
    "Support Vector Machine (SVM): Trained with default parameters.\n",
    "\n",
    "Predictions and Evaluation\n",
    "Making Predictions: Each classifier makes predictions on the test set.\n",
    "Evaluating Performance:\n",
    "Accuracy: The accuracy score is computed for each classifier.\n",
    "Confusion Matrix: The confusion matrix is printed for each classifier to understand the distribution of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "\n",
    "Explanation of Models\n",
    "Random Forest Classifier:\n",
    "    Model: An ensemble learning method that builds multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    "    Hyperparameters Tuned:\n",
    "        n_estimators: Number of trees in the forest.\n",
    "        max_depth: Maximum depth of the tree.\n",
    "        min_samples_split: Minimum number of samples required to split an internal node.\n",
    "Gradient Boosting Classifier:\n",
    "    Model: An ensemble technique that builds models sequentially, each new model correcting errors made by the previous ones.\n",
    "    Hyperparameters: Used default parameters.\n",
    "Support Vector Machine (SVM):\n",
    "    Model: A supervised learning model that finds the hyperplane that best separates the classes in the feature space.\n",
    "    Hyperparameters: Used default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af481a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Random Forest Accuracy: 0.9945054945054945\n",
      "Gradient Boosting Machine Accuracy: 0.978021978021978\n",
      "Support Vector Machine Accuracy: 0.9945054945054945\n",
      "Confusion Matrix for Random Forest:\n",
      "[[20  0  0  0  0  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Confusion Matrix for Gradient Boosting Machine:\n",
      "[[17  0  0  1  2  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Confusion Matrix for Support Vector Machine:\n",
      "[[20  0  0  0  0  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\") \n",
    "\n",
    "text_field = \"role_description\"  \n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "# Train other classifiers without hyperparameter tuning\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# model performance\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097e4ab",
   "metadata": {},
   "source": [
    "# Model Performance -\n",
    "\n",
    "Based on the accuracy scores and confusion matrices provided for the Random Forest, Gradient Boosting Machine (GBM), and Support Vector Machine (SVM) classifiers, we can make several observations about their performance.\n",
    "\n",
    "Accuracy\n",
    "    Random Forest Accuracy: 99.45%\n",
    "    Gradient Boosting Machine Accuracy: 97.80%\n",
    "    Support Vector Machine Accuracy: 99.45%\n",
    "    \n",
    "The Random Forest and SVM models have the same high accuracy, both outperforming the GBM model slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d6b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hsahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Random Forest Accuracy: 0.9945054945054945\n",
      "Gradient Boosting Machine Accuracy: 0.978021978021978\n",
      "Support Vector Machine Accuracy: 0.9945054945054945\n",
      "Confusion Matrix for Random Forest:\n",
      "[[20  0  0  0  0  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Confusion Matrix for Gradient Boosting Machine:\n",
      "[[17  0  0  1  2  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n",
      "Confusion Matrix for Support Vector Machine:\n",
      "[[20  0  0  0  0  0  0  0  1]\n",
      " [ 0 19  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"C:/Users/hsahn/Downloads/job_details.csv\")\n",
    "\n",
    "text_field = \"role_description\"\n",
    "\n",
    "# Drop rows with missing values in the 'role_description' column\n",
    "data.dropna(subset=[text_field], inplace=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "job_domains = {\n",
    "    \"Software Development\": [\n",
    "        \"android\", \"backend\", \"full stack\", \"node.js\", \"python\", \"web developer\",\n",
    "        \"elixir\", \"phoenix\", \"sde\", \"react native\", \"software developer\", \"java\", \n",
    "        \"kotlin\", \"jetpack compose\", \"sdk\", \"firebase\"\n",
    "    ],\n",
    "    \"Data Science\": [\n",
    "        \"data analyst\", \"data scientist\", \"big data\", \"machine learning\",\n",
    "        \"data analytics\", \"prompt engineer\", \"mlops\", \"data analysis\",\n",
    "        \"ai\", \"artificial intelligence\", \"statistical modeling\", \"deep learning\"\n",
    "    ],\n",
    "    \"Marketing\": [\n",
    "        \"marketing\", \"brand marketing\", \"digital marketing\", \"social media\",\n",
    "        \"influencer\", \"content creation\", \"seo\", \"email marketing\",\n",
    "        \"product marketing\", \"advertising\", \"market research\"\n",
    "    ],\n",
    "    \"Human Resources\": [\n",
    "        \"human resource\", \"hr\", \"recruitment\", \"talent acquisition\",\n",
    "        \"comp & benefits\", \"employee relations\", \"training\", \"development\"\n",
    "    ],\n",
    "    \"Sales\": [\n",
    "        \"sales\", \"business development\", \"inside sales\", \"account manager\",\n",
    "        \"lead generation\", \"sales executive\", \"territory manager\"\n",
    "    ],\n",
    "    \"Operations\": [\n",
    "        \"operations\", \"business operations\", \"supply chain\", \"logistics\",\n",
    "        \"inventory management\", \"procurement\", \"project management\"\n",
    "    ],\n",
    "    \"Research\": [\n",
    "        \"research\", \"insights\", \"data analysis\", \"market research\",\n",
    "        \"academic research\", \"clinical research\", \"r&d\"\n",
    "    ],\n",
    "    \"Product Management\": [\n",
    "        \"product management\", \"product solution\", \"product architect\",\n",
    "        \"product owner\", \"product strategy\", \"product development\"\n",
    "    ],\n",
    "    \"Engineering\": [\n",
    "        \"robotics\", \"unity\", \"climate\", \"ai\", \"automotive\",\n",
    "        \"mechanical\", \"steering\", \"suspension\", \"brakes\",\n",
    "        \"civil engineering\", \"electrical engineering\", \"chemical engineering\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# assign job domains based on skills\n",
    "def assign_domain(text):\n",
    "    text = text.lower()\n",
    "    for domain, skills in job_domains.items():\n",
    "        if any(skill in text for skill in skills):\n",
    "            return domain\n",
    "    return \"Other\"\n",
    "\n",
    "# domains based on job descriptions\n",
    "data[\"domain\"] = data[text_field].apply(assign_domain)\n",
    "\n",
    "# text cleaning\n",
    "def clean_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char.isalnum() or char in \" \"])\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"cleaned_text\"] = data[text_field].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Drop rows with empty cleaned_text\n",
    "data = data[data[\"cleaned_text\"].str.strip() != \"\"]\n",
    "\n",
    "# features and target\n",
    "X = data[\"cleaned_text\"]\n",
    "y = data[\"domain\"]\n",
    "\n",
    "# Vectorize text data using bi-grams\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_vec, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "\n",
    "# Train other classifiers without hyperparameter tuning\n",
    "gbm_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "rf_predictions = best_rf_classifier.predict(X_test)\n",
    "gbm_predictions = gbm_classifier.predict(X_test)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# model performance\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Machine Accuracy:\", gbm_accuracy)\n",
    "print(\"Support Vector Machine Accuracy:\", svm_accuracy)\n",
    "\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, rf_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Gradient Boosting Machine:\")\n",
    "print(confusion_matrix(y_test, gbm_predictions))\n",
    "\n",
    "print(\"Confusion Matrix for Support Vector Machine:\")\n",
    "print(confusion_matrix(y_test, svm_predictions))\n",
    "\n",
    "# Save the trained models and vectorizer\n",
    "joblib.dump(best_rf_classifier, 'best_random_forest_classifier.joblib')\n",
    "joblib.dump(gbm_classifier, 'gbm_classifier.joblib')\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load the models and vectorizer (for future use)\n",
    "# best_rf_classifier = joblib.load('best_random_forest_classifier.joblib')\n",
    "# gbm_classifier = joblib.load('gbm_classifier.joblib')\n",
    "# svm_classifier = joblib.load('svm_classifier.joblib')\n",
    "# vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Predict domains for the original dataset\n",
    "data[\"predicted_domain\"] = best_rf_classifier.predict(vectorizer.transform(data[\"cleaned_text\"]))\n",
    "\n",
    "# Save the dataset with predictions to a new CSV file\n",
    "data.to_csv(\"C:/Users/hsahn/Downloads/job_details_with_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d58ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
